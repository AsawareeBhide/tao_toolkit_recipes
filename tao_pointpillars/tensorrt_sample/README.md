# PointPillar TensorRT Inference Sample
TensorRT Inference Sample for PointPillars in NVIDIA TAO Toolkit

# PointPillars inference with TensorRT
This repository provides an end-to-end inference sample for [PointPillars](https://arxiv.org/abs/1812.05784) with TensorRT.

The input model is the TensorRT engine generated by NVIDIA TAO toolkit with `tao-converter`.

## Detailed Steps

* Install TensorRT 8.2(or above)

* Install TensorRT OSS 22.02
```
git clone -b 22.02 https://github.com/NVIDIA/TensorRT.git TensorRT
cd TensorRT
git submodule update --init --recursive
mkdir -p build && cd build
cmake .. -DCUDA_VERSION=$CUDA_VERSION -DGPU_ARCHS=$GPU_ARCHS
make nvinfer_plugin -j$(nproc)
make nvinfer_plugin_static -j$(nproc)
cp libnvinfer_plugin.so.8.2.* /usr/lib/$ARCH-linux-gnu/libnvinfer_plugin.so.8.2.3
cp libnvinfer_plugin_static.a /usr/lib/$ARCH-linux-gnu/libnvinfer_plugin_static.a
```

* Train and export the `.etlt` model with TAO Toolkit

* Generate TensorRT engine with `tao-converter`

```
tao-converter  -k $KEY  \
               -e $USER_EXPERIMENT_DIR/trt.fp16.engine \
               -p points,1x204800x4,1x204800x4,1x204800x4 \
               -p num_points,1,1,1 \
               -t fp16 \
               $USER_EXPERIMENT_DIR/pointpillars_deployable.etlt
```

* Clone the repo

```
cd ~
git clone https://github.com/NVIDIA-AI-IOT/tao_toolkit_recipes.git
cd tao_toolkit_recipes
git lfs pull
```

* Run the TensorRT Inference

```
cd tao_pointpillars/tensorrt_sample/test
mkdir build
cd build
cmake .. -DCUDA_VERSION=<CUDA_VERSION>
make -j8
./pointpillars -e /path/to/tensorrt/engine -l ../../data/102.bin  -t 0.01 -c Vehicle,Pedestrain,Cyclist -n 4096 -p -d fp16
```
